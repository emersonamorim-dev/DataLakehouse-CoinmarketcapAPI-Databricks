# Data Lakehouse Coinmarketcap - Databricks

Codifica√ß√£o em Python com Framework Streamlit e Flask este aplica√ß√£o completa de Data Lakehouse usando a API da Coinmarketcap usando o  Databricks extrair os dados. Essa aplica√ß√£o foi projetado para processar e visualizar dados em uma arquitetura Data Lakehouse multicamadas usando Streamlit para Dashboard, MongoDB como banco de dados e um pipeline estruturado para processos ETL (Extrair, Transformar, Carregar).

## Estrutura de diret√≥rio

Aqui est√° uma vis√£o geral da estrutura de diret√≥rios:

- `airflow/` - Cont√©m DAGs Apache Airflow para gerenciamento de fluxo de trabalho.
- `dashboard/` - Arquivos do painel Streamlit.
- `dados/`
   - `bronze/` - Dados brutos ingeridos no data lake.
   - `silver/` - Dados intermedi√°rios processados.
   - `gold/` - Dados finais processados prontos para an√°lise.
- `databricks/`
   - `jobs/` - Configura√ß√µes de trabalho para Databricks.
   - `notebooks/` - Notebooks Databricks para processamento de dados.
- `datalake/` - Cont√©m camadas de dados estruturados correspondentes a bronze, prata e ouro.
- `docker/` - Arquivos Docker para conteineriza√ß√£o.
- `env/` - Arquivos de ambiente para configura√ß√£o.
- `jupyter/notebook/` - Notebooks Jupyter para processamento e explora√ß√£o de dados.
- `kubernetes/` - Arquivos de configura√ß√£o do Kubernetes.
- `logs/` - Logs do aplicativo e servi√ßos.
- `pipeline/`
   - `agregado/` - Scripts de agrega√ß√£o.
   - `extract/` - Scripts de extra√ß√£o de dados.
   - `transform/` - Scripts de transforma√ß√£o de dados.
- `powerbi/` - Arquivos do painel do Power BI.


### Tecnologias Utilizadas üõ†Ô∏è

![Apache Airflow](https://img.shields.io/badge/-Apache%20Airflow-017CEE?style=for-the-badge&logo=apache-airflow&logoColor=white)
![Databricks](https://img.shields.io/badge/-Databricks-FF3621?style=for-the-badge&logo=databricks&logoColor=white)
![Jupyter](https://img.shields.io/badge/-Jupyter-F37626?style=for-the-badge&logo=jupyter&logoColor=white)
![Kubernetes](https://img.shields.io/badge/-Kubernetes-326CE5?style=for-the-badge&logo=kubernetes&logoColor=white)
![Docker](https://img.shields.io/badge/-Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![Python](https://img.shields.io/badge/-Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![PowerBI](https://img.shields.io/badge/-PowerBI-F2C811?style=for-the-badge&logo=power-bi&logoColor=black)
![MongoDB](https://img.shields.io/badge/-MongoDB-47A248?style=for-the-badge&logo=mongodb&logoColor=white)
![Streamlit](https://img.shields.io/badge/-Streamlit-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white)

## Come√ßando

### Instala√ß√£o

√â necess√°rio alterar os caminhos em main.py
```
/home/your-user/seu-diretorio/data-lakehouse-coinmarketcap/datalake/bronze/bronze.json
```

Em aggregate do arquivo gold.py
```
/home/your-user/seu-diretorio/data-lakehouse-coinmarketcap/datalake/gold/gold.json
```

1. Clone o reposit√≥rio em sua m√°quina local.
2. Navegue at√© o diret√≥rio e crie um ambiente virtual:

- Rode os seguintes comandos:

```
python -m venv venv
```
```
source venv/bin/activate
```

3. Instale os pacotes Python necess√°rios:

```
pip install -r requirements.txt
```

4. Inicie o servi√ßo MongoDB em sua m√°quina ou use o Docker para executar um cont√™iner MongoDB.

### Executando o aplicativo

1. √â necess√°rio iniciar sua inst√¢ncia do MongoDB se ainda n√£o estiver em execu√ß√£o para rodar aplica√ß√£o:
Adicione o reposit√≥rio oficial do MongoDB: Primeiro, voc√™ precisa adicionar a chave GPG do reposit√≥rio oficial do MongoDB e adicionar o reposit√≥rio aos seus reposit√≥rios APT. Execute os seguintes comandos no terminal:

```
wget -qO - https://www.mongodb.org/static/pgp/server-5.0.asc | sudo apt-key add -
```

```
echo "deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu focal/mongodb-org/5.0 multiverse" | sudo tee /etc/apt/sources.list.d/mongodb-org-5.0.list
```

```
sudo apt update
```

```
sudo apt install -y mongodb-org
```

```
sudo systemctl start mongod
```

```
sudo systemctl status mongod
```

O MongoDB deve aparecer em verde com seguinte mensagem:
Active: active (running)

- Agora abra outra Aba do Termonal √© necesss√°rio criar o Banco de dados no Terminal e criar a Collection:

```
mongo
```

Criar um banco de dados: No shell do MongoDB, para criar um novo banco de dados, voc√™ pode simplesmente especificar o nome do banco de dados da seguinte maneira:

```
use coinDB
```

Criar uma cole√ß√£o: Ap√≥s selecionar um banco de dados, voc√™ pode criar uma cole√ß√£o dentro desse banco de dados usando o seguinte comando:

```
db.createCollection("coinMarket")
```

Depois de executar esses comandos, voc√™ ter√° criado um banco de dados e uma cole√ß√£o dentro desse banco de dados. Voc√™ pode verificar se eles foram criados corretamente usando comandos como show dbs para listar os bancos de dados e show collections para listar as cole√ß√µes dentro de um banco de dados espec√≠fico.

listar os bancos de dados:

```
show dbs
```

listar as cole√ß√µes

```
show collections
```

2. Para executar o Dashbord no Streamlit, navegue at√© o diret√≥rio `dashboard/` configure o Banco de dados criado
no terminal e a collection para depois ir executar os comandos para rodar Dashboard

```
streamlit run dashboard.py
```

```
streamlit run app.py
```
3. Acesse o Dashbord do Streamlit em seu navegador em `localhost:8501`.


4. Para rodar o Apache Airflow dentro de uma aplica√ß√£o abra uma nova aba no Terminal no diret√≥rio da aplica√ß√£o:
A estrutura de diret√≥rios na imagem indica que h√° um ambiente virtual do Python configurado (airflow_venv), diret√≥rios para DAGs (dags), plugins personalizados (plugins), scripts (scripts) e um banco de dados SQLite (sqlite).


1. Ativar o Ambiente Virtual
Primeiro, voc√™ deve ativar o ambiente virtual que cont√©m o Apache Airflow e suas depend√™ncias. Abra um terminal e navegue at√© o diret√≥rio que cont√©m o airflow_venv e execute o seguinte comando:

```
python -m venv venv
```

```
source venv/bin/activate
```

2. Inicializar o Banco de Dados do Airflow
Antes de executar o Airflow pela primeira vez, √© necess√°rio inicializar seu banco de dados. Com a estrutura de diret√≥rios fornecida, parece que voc√™ est√° usando um banco de dados SQLite. O comando de inicializa√ß√£o √©:

```
airflow db init
```

3. Criar um Usu√°rio (Se Necess√°rio)
Para acessar a interface do usu√°rio (UI) do Airflow, voc√™ precisa criar um usu√°rio. Voc√™ pode fazer isso com o seguinte comando:

```
airflow users create \
    --username your_username \
    --firstname Your_Firstname \
    --lastname Your_Lastname \
    --role Admin \
    --email your_email@example.com
```

4. Iniciar o Scheduler do Airflow
O Scheduler do Airflow √© o componente que orquestra a execu√ß√£o das DAGs. Para inici√°-lo, execute:

```
airflow scheduler
```

5. Iniciar a Webserver do Airflow
Em um novo terminal (certificando-se de que o ambiente virtual esteja ativado), inicie a webserver do Airflow com:

```
airflow webserver --port 8088
```

Isso iniciar√° a interface web do Airflow na porta 8080, e voc√™ poder√° acess√°-la atrav√©s de um navegador de internet navegando at√© http://localhost:8088.

6. Testar e Ativar as DAGs
Com a interface web do Airflow funcionando, voc√™ pode testar e ativar suas DAGs:

Testar uma DAG: No terminal, voc√™ pode testar uma tarefa individual usando o comando airflow tasks test, por exemplo:

```
airflow tasks test your_dag_id your_task_id your_execution_date
```

Ativar/Desativar uma DAG: Na interface web, clique na DAG que deseja ativar e alterne o interruptor para o estado 'ON'.


7. O c√≥digo acima assume que voc√™ tem um MongoDB rodando localmente e cont√©m c√≥digo para puxar dados dele. Ajuste a string de conex√£o e os nomes do banco de dados e da cole√ß√£o conforme necess√°rio.
Plotly Express:

Estamos usando Plotly Express para criar um gr√°fico de linha simples. Voc√™ pode ajustar isso conforme necess√°rio para se adequar aos seus dados e requisitos.
Layout Dash:

O layout do Dash √© definido usando uma combina√ß√£o de componentes HTML e core. Voc√™ pode expandir isso para incluir mais gr√°ficos, tabelas, filtros e outros elementos conforme necess√°rio.
Execu√ß√£o:

Execute o aplicativo usando python app.py, e ele ser√° hospedado localmente. Voc√™ pode acess√°-lo em http://127.0.0.1:8050 no seu navegador web.
Personaliza√ß√£o:

Este √© um exemplo b√°sico. O Dash √© altamente personaliz√°vel, e voc√™ pode adicionar muitos outros elementos, estilos e interatividades ao seu dashboard.
Dados:

Certifique-se de que seus dados est√£o estruturados e limpos para serem visualizados. Voc√™ pode precisar ajustar o c√≥digo de acordo com a estrutura espec√≠fica dos seus dados.
Componentes:

Explore os componentes do Dash e do Plotly para adicionar funcionalidades como dropdowns, sliders, tabelas e outros elementos interativos ao seu dashboard.

### Processamento de dados

1. Use os notebooks Jupyter fornecidos em `jupyter/notebook/` para executar tarefas ETL:
- `etl_bronze.ipynb`: para processar dados brutos para a camada bronze.
- `etl_silver.ipynb`: para refinar dados de bronze para a camada de prata.
- `etl_gold.ipynb`: para processamento final para obten√ß√£o de dados da camada de ouro.

2. Agende e monitore fluxos de trabalho ETL usando Apache Airflow localizado no diret√≥rio `airflow/`.

### Implanta√ß√£o via Containers

- Para implanta√ß√£o no Kubernetes, consulte as configura√ß√µes no diret√≥rio `kubernetes/`.
- Use os arquivos do diret√≥rio `docker/` para construir e executar cont√™ineres para configura√ß√£o de ambiente isolado.


### Conclus√£o
Este sistema completo de Data Lakehouse da CoinMarketCap o projeto foi uma iniciativa ambiciosa que visou capitalizar a riqueza de dados do mercado de criptomoedas para fornecer insights aprofundados e aprimorar a tomada de decis√£o no ambiente financeiro din√¢mico de hoje. Utilizando a robusta plataforma do Databricks, conseguimos consolidar, processar e analisar grandes volumes de dados relacionados a pre√ßos de ativos, volumes de transa√ß√£o, e mudan√ßas de mercado em tempo real.

O Databricks serviu como o n√∫cleo de processamento e an√°lise, permitindo-nos executar transforma√ß√µes complexas de dados e algoritmos de aprendizado de m√°quina em um ambiente unificado. O uso de notebooks interativos facilitou a colabora√ß√£o entre analistas de dados, engenheiros de dados e cientistas de dados, enquanto a infraestrutura gerenciada simplificou o provisionamento e a manuten√ß√£o de recursos computacionais.

Por meio de nosso Data Lakehouse, estabelecemos camadas de dados categorizadas como bronze, prata e ouro, que representam respectivamente os dados brutos, processados e prontos para an√°lise. Esta abordagem estratificada assegurou que os dados fossem refinados de maneira sistem√°tica, maximizando a confiabilidade e a utilidade das informa√ß√µes geradas.

Os dashboards interativos criados com Streamlit proporcionaram uma interface intuitiva para a explora√ß√£o de dados, permitindo aos usu√°rios finais acessar relat√≥rios customizados e realizar an√°lises ad-hoc com facilidade. Al√©m disso, a integra√ß√£o com MongoDB ofereceu uma solu√ß√£o √°gil para armazenar e recuperar dados operacionais.

Atrav√©s desta solu√ß√£o de Data Lakehouse, entregamos uma plataforma que n√£o s√≥ suporta o crescimento cont√≠nuo do volume e da variedade de dados do CoinMarketCap mas tamb√©m introduz a escalabilidade necess√°ria para futuras expans√µes. As potenciais √°reas de crescimento incluem a incorpora√ß√£o de novas fontes de dados, o aprimoramento de modelos de previs√£o de pre√ßos com t√©cnicas avan√ßadas de machine learning e a expans√£o para suportar outras classes de ativos al√©m das criptomoedas.

Em resumo, o projeto Data Lakehouse CoinMarketCap com Databricks foi uma jornada transformadora que fortaleceu nossa capacidade anal√≠tica e ofereceu uma vis√£o valiosa sobre o mercado de criptomoedas. Continuaremos a evoluir nossa plataforma para manter a relev√¢ncia e o valor em um mercado que est√° sempre mudando.



### Desenvolvido por:
Emerson Amorim [@emerson-amorim-dev](https://www.linkedin.com/in/emerson-amorim-dev/)





